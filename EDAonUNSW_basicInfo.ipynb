{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luseewhy/UNSW_EDA/blob/main/EDAonUNSW_basicInfo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3vqngEj4ExR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_HxLYzo4Kdr"
      },
      "source": [
        "# **Intrusion Detection Systems**\n",
        "IDS are of two types: misuse-based IDS and the anomalybased IDS (Aydin, Zaim, & Ceylan, 2009). In misusebased IDS, the attack signatures of new connections in a\n",
        "network are compared with the existing attack signatures\n",
        "to determine whether the new connection is malicious or\n",
        "not. Misuse based IDS are useful only in detecting the\n",
        "known attacks. What is more, dangerous are the latest\n",
        "attacks or the \"zeroday\" attacks, which are never seen\n",
        "before (Boutaba, et al., 2018). Here, anomalybased\n",
        "intrusion detection systems come into the picture. The\n",
        "basic idea of anomalybased intrusion detection system is\n",
        "to check for the normal behavior of the network and\n",
        "report any deviation from the expected. Anomaly-based\n",
        "IDS can be implemented using various machine learning\n",
        "approaches.\n",
        "\n",
        "# **UNSW-NB15 > KDD**\n",
        "\n",
        "The UNSW-NB15 dataset (The UNSW-NB15\n",
        "Dataset Description, 2018), an upgraded version of the\n",
        "KDD cup dataset. This dataset helps us analyze the\n",
        "features of a healthy network and a network under attack.\n",
        "UNSW-NB15 is a modern, more balanced dataset and is\n",
        "becoming the new benchmark dataset for building robust\n",
        "intrusion detection models(IDS).\n",
        "This dataset is class wise more\n",
        "balanced as compared to other datasets available for\n",
        "intrusion detection. The UNSW-NB15 dataset although\n",
        "less in size as compared to others, but even less\n",
        "redundant, is sufficient to train a model with high\n",
        "accuracy.\n",
        "\n",
        "CLASSES IN UNSW-NB15: The\n",
        "UNSW-NB15 dataset contains ten classes, namely:\n",
        "Normal, Fuzzers, Analysis, Back- doors, DoS, Exploits,\n",
        "Generic, Reconnaissance, Shell Code, and Worms.\n",
        "\n",
        "#EDA\n",
        "Exploratory data analysis is the process in which we use\n",
        "data visualization techniques to analyze and find patterns\n",
        "in the data so to have a better understanding of the\n",
        "features. Through EDA we get insights of how the data is\n",
        "distributed, how one feature is correlated with the other\n",
        "and what anomalies are there in the dataset. Based on the\n",
        "insights, we make future decisions on what could be the\n",
        "optimum features to include in the dataset while training\n",
        "or what relevant features could be extracted for improving\n",
        "the accuracy of the model.\n",
        "\n",
        "#*Dataset Used*\n",
        "Binary version of dataset is also\n",
        "available where 0 describes normal and 1 for attack.\n",
        "There are total 45 features in the dataset which helps in accurately classifying the targets. The size of training data is 175,341 samples\n",
        "whereas 82,332 is the size of the test data.\n",
        "\n",
        "https://eudl.eu/pdf/10.4108/eai.13-10-2021.171319--> refer this link to get the whole information about the basics.( read after data preprocessing topic)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xvlb561nBLm"
      },
      "outputs": [],
      "source": [
        "# DATA PRE-PROCESSING\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7qq3WkVpt2A",
        "outputId": "dfc50522-f120-430b-9197-23574a980822",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/UNSW_NB15_testing-set.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-40d169a4ce3e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/UNSW_NB15_testing-set.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/UNSW_NB15_testing-set.csv'"
          ]
        }
      ],
      "source": [
        "dataset=pd.read_csv('/content/UNSW_NB15_testing-set.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "ig5ImOO_qFs8",
        "outputId": "4ef10fa9-549a-407e-ae9d-a084221d34d9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3075b36c09c8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "r-7Wd0eBqHhg",
        "outputId": "4e9c87d4-7ee0-48ff-e4a8-2006e18e5040"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8bef6b4f3569>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;31m#(rows,columns) format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "dataset.shape #(rows,columns) format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "1-sEZUWzqWe4",
        "outputId": "ec718405-ac32-4351-b579-e3420338db40"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-d5abdb5e5fbe>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Cleaning dataset by removing the columns with no values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "# Cleaning dataset by removing the columns with no values\n",
        "dataset = dataset.dropna(axis=1)\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCXCxYbH7pKr"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "# WE ARE CONVERTNG CATEGORICAL DATA INTO NUMBERICAL DATA\n",
        "categorical_column = 'proto'  # Replace with your actual column name\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit and transform the specified column\n",
        "dataset[categorical_column] = label_encoder.fit_transform(dataset[categorical_column])\n",
        "\n",
        "dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "ih2ZYzjJ850t",
        "outputId": "7c923201-4d25-48c6-9f2f-9aad31c6c1d4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9e1c33820f1b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcategorical_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'state'\u001b[0m  \u001b[0;31m# Replace with your actual column name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabel_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_column\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategorical_column\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcategorical_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'attack_cat'\u001b[0m  \u001b[0;31m# Replace with your actual column name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "categorical_column = 'state'  # Replace with your actual column name\n",
        "label_encoder = LabelEncoder()\n",
        "dataset.loc[:, categorical_column] = label_encoder.fit_transform(dataset[categorical_column])\n",
        "\n",
        "categorical_column = 'attack_cat'  # Replace with your actual column name\n",
        "label_encoder = LabelEncoder()\n",
        "dataset.loc[:, categorical_column] = label_encoder.fit_transform(dataset[categorical_column])\n",
        "\n",
        "print(dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1Ln16nK9L4I"
      },
      "outputs": [],
      "source": [
        "#  Now we will remove the outliers from the data\n",
        "df=dataset\n",
        "# Identify numerical columns in the dataset\n",
        "numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "# Calculate z-scores for each data point in numerical columns\n",
        "z_scores = np.abs((df[numeric_columns] - df[numeric_columns].mean()) / df[numeric_columns].std())\n",
        "\n",
        "# Define a threshold for outlier detection (e.g., z_score_threshold = 3)\n",
        "z_score_threshold = 3\n",
        "\n",
        "# Find and remove rows with outliers\n",
        "outlier_rows = z_scores[(z_scores > z_score_threshold).any(axis=1)].index\n",
        "df_no_outliers = df.drop(outlier_rows)\n",
        "\n",
        "# Alternatively, you can replace outliers with NaN values\n",
        "# df_no_outliers = df.copy()\n",
        "# df_no_outliers[numeric_columns][(z_scores > z_score_threshold)] = np.nan\n",
        "# df_no_outliers = df_no_outliers.dropna()\n",
        "\n",
        "# Print the shape of the original and cleaned datasets\n",
        "print(\"Original dataset shape:\", df.shape)\n",
        "print(\"Dataset shape after removing outliers:\", df_no_outliers.shape)\n",
        "\n",
        "df=df_no_outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ApA4j5A_knQ"
      },
      "outputs": [],
      "source": [
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTaw41iFA4Ep"
      },
      "outputs": [],
      "source": [
        "independent=df[['id', 'dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes',\n",
        "       'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss',\n",
        "       'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin',\n",
        "       'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
        "       'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
        "       'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
        "       'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
        "       'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat']].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFK86AKZB2hn"
      },
      "outputs": [],
      "source": [
        "dependent=df[['label']]\n",
        "independent\n",
        "# Now I have converted the data into array (multidimensional) with one array consisting of independent data and the other containing dependent data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqYwtH5uB93q"
      },
      "outputs": [],
      "source": [
        "# VISUALIZATION\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "# Plot correlation heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvfVY-mqMxQk"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Specify the prominent features for the boxplot\n",
        "prominent_features = ['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss']\n",
        "\n",
        "# Create a box and whisker plot for the selected features\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.boxplot(data=df[prominent_features])\n",
        "plt.title('Box and Whisker Plot for 6 Prominent Features')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cr2ostzsNs2K"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Specify the six most prominent features for the violin plot\n",
        "prominent_features = ['dur', 'sbytes', 'dbytes', 'sttl', 'dttl', 'sloss']\n",
        "\n",
        "# Create a violin plot for the selected features\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.violinplot(data=df[prominent_features])\n",
        "plt.title('Violin Plot for 6 Most Prominent Features')\n",
        "plt.xlabel('Features')\n",
        "plt.ylabel('Values')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAIIZe-3frrj"
      },
      "outputs": [],
      "source": [
        "df_testing=pd.read_csv('/content/UNSW_NB15_testing-set.csv')\n",
        "df_testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cTGmpNMOJa1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Assuming df_training and df_testing are your original DataFrames\n",
        "df_training = pd.read_csv('/content/UNSW_NB15_training-set.csv')\n",
        "df_testing = pd.read_csv('/content/UNSW_NB15_testing-set.csv')\n",
        "\n",
        "# Selecting features and labels for training data\n",
        "independent_train = df_training[['id', 'dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes',\n",
        "                                'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss',\n",
        "                                'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin',\n",
        "                                'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
        "                                'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
        "                                'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
        "                                'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
        "                                'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat']]\n",
        "dependent_train = df_training[['label']]\n",
        "\n",
        "# Selecting features and labels for testing data\n",
        "independent_test = df_testing[['id', 'dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes',\n",
        "                               'dbytes', 'rate', 'sttl', 'dttl', 'sload', 'dload', 'sloss', 'dloss',\n",
        "                               'sinpkt', 'dinpkt', 'sjit', 'djit', 'swin', 'stcpb', 'dtcpb', 'dwin',\n",
        "                               'tcprtt', 'synack', 'ackdat', 'smean', 'dmean', 'trans_depth',\n",
        "                               'response_body_len', 'ct_srv_src', 'ct_state_ttl', 'ct_dst_ltm',\n",
        "                               'ct_src_dport_ltm', 'ct_dst_sport_ltm', 'ct_dst_src_ltm',\n",
        "                               'is_ftp_login', 'ct_ftp_cmd', 'ct_flw_http_mthd', 'ct_src_ltm',\n",
        "                               'ct_srv_dst', 'is_sm_ips_ports', 'attack_cat']]\n",
        "dependent_test = df_testing[['label']]\n",
        "\n",
        "# Splitting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(independent_train, dependent_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Defining columns to one-hot encode\n",
        "categorical_columns = ['proto', 'service', 'state', 'attack_cat']\n",
        "\n",
        "# Creating a column transformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(), categorical_columns)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Creating pipelines for each model\n",
        "rf_model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', RandomForestClassifier())\n",
        "])\n",
        "\n",
        "svm_model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', SVC())\n",
        "])\n",
        "\n",
        "lr_model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression())\n",
        "])\n",
        "\n",
        "# Training models\n",
        "rf_model.fit(X_train, y_train.values.ravel())\n",
        "svm_model.fit(X_train, y_train.values.ravel())\n",
        "lr_model.fit(X_train, y_train.values.ravel())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ-YFKjBYoNn"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Make predictions\n",
        "rf_predictions = rf_model.predict(X_test)\n",
        "svm_predictions = svm_model.predict(X_test)\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "rf_accuracy = accuracy_score(y_test, rf_predictions)\n",
        "svm_accuracy = accuracy_score(y_test, svm_predictions)\n",
        "lr_accuracy = accuracy_score(y_test, lr_predictions)\n",
        "\n",
        "# Display accuracies\n",
        "print(f\"Random Forest Accuracy: {rf_accuracy:.4f}\")\n",
        "print(f\"SVM Accuracy: {svm_accuracy:.4f}\")\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfD8PVt-q94f"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Instantiate Naive Bayes and KNN models\n",
        "nb_model = GaussianNB()\n",
        "knn_model = KNeighborsClassifier(n_neighbors=5)  # You can adjust the number of neighbors\n",
        "\n",
        "# Train Naive Bayes and KNN models\n",
        "nb_model.fit(X_train, y_train)\n",
        "knn_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "nb_predictions = nb_model.predict(X_test)\n",
        "knn_predictions = knn_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "nb_accuracy = accuracy_score(y_test, nb_predictions)\n",
        "knn_accuracy = accuracy_score(y_test, knn_predictions)\n",
        "\n",
        "# Display accuracies\n",
        "print(f\"Naive Bayes Accuracy: {nb_accuracy:.4f}\")\n",
        "print(f\"KNN Accuracy: {knn_accuracy:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}